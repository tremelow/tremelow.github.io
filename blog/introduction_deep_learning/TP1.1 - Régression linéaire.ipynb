{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet 1, TP1 - Introduction à la régression linéaire\n",
    "\n",
    "Au cœur du deep learning sont des **fonctions** :\n",
    "\n",
    "- image d'un chiffre ➡ le chiffre en question (MNIST)\n",
    "- labels & bruit blanc ➡ image (modèles de diffusion, e.g. Midjourney)\n",
    "- fragment de texte ➡ texte complété (auto-régression, e.g. ChatGPT)\n",
    "- texte dans une langue ➡ texte dans une autre langue (transformers, e.g. [DeepL](https://www.deepl.com/en/blog/how-does-deepl-work))\n",
    "\n",
    "Le principe est qu'on ne connaît pas *a priori* cette fonction : on la cherche avec, par exemple, un réseau de neurones qui doit «apprendre».\n",
    "Cette procédure d'apprentissage est [caractérisé par trois choix](https://ml-course.github.io/master/notebooks/01%20-%20Introduction.html#learning-representation-evaluation-optimization) :\n",
    "\n",
    "1. **Représentation :** Un modèle qui peut être implémenté (l'architecture, le nombre de paramètres...) ;\n",
    "2. **Évaluation :** Une *loss*, une fonction objectif ou de score, pour évaluer les paramètres ;\n",
    "3. **Optimisation :** Une manière *efficace* d'ajuster les paramètres.\n",
    "\n",
    "Dans ce cours, on s'intéresse à **l'apprentissage supervisé**, c'est-à-dire qu'on connait les images de la fonction (les *outputs*) pour certains arguments (certains *inputs*). Ainsi, la partie d'évaluation consistera à vérifier que les *outputs* correspondent dans notre jeu de données.\n",
    "Pour la représentation, on va commencer par un simple modèle de régression linéaire. On étudiera ensuite deux types de réseaux de neurones : les [perceptrons multi-couches](https://fr.wikipedia.org/wiki/Perceptron_multicouche) et les [transformeurs](https://fr.wikipedia.org/wiki/Transformeur).\n",
    "\n",
    "Ce premier projet, séparé en 3 TPs, se concentre sur **l'optimisation**. C'est parce que cet élément est complexe qu'on considère un exemple simple à 2 paramètres interprétables, de sorte à mettre la représentation et l'évaluation de côté pour le moment. On va d'abord rapidement introduire le problème pour se motiver, puis introduire des méthodes d'optimisation avec un seul paramètre et enfin introduire un ingrédient majeur du deep learning : la descente de gradient stochastique. Cela permettra aussi de nous acclimater au pipeline PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quelques imports utiles\n",
    "import numpy as np\n",
    "from numpy.polynomial.polynomial import Polynomial\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "rng_seed = sum(ord(c) ** 2 for c in \"R5.A.12-ModMath\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 1 - Introduction du problème de régression linéaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Génération du jeu de données\n",
    "\n",
    "Cette fonction génère un jeu de données (ou *dataset*), qui consiste en des couples *input, output*. \n",
    "Ici, la fonction sous-jacente est probabiliste,\n",
    "$$ y^{(k)} = f(x^{(k)}) = w x^{(k)} + b + \\varepsilon_k , $$\n",
    "où tous les $\\varepsilon_k$ sont tirés aléatoirement et indépendemment, de même loi normale $\\varepsilon_k \\sim \\mathcal{N}(0, \\sigma^2)$. \n",
    "On n'a pas accès aux $\\varepsilon_k$, seulement aux couples $(x^{(k)}, y^{(k)})$. \n",
    "Les paramètres $w$ et $b$ s'appellent respectivement la pente (*slope*) et le biais (*bias*).\n",
    "\n",
    "Par exemple, les données peuvent représenter la taille $x$ et le poids $y$ de différents individus. Il y a des variations intrinsèques que l'on modélise par de l'aléatoire : deux individus de même taille peuvent avoir un poids différent. On regarde ici un problème générique, donc les données ici sont purement fictives, ne représentent rien de particulier, et sont générées automatiquement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(\n",
    "    params=(5.0, -2.0),\n",
    "    x_span=(-3.0, 3.0),\n",
    "    n_data=100,\n",
    "    noise=6.0,\n",
    "    rng=np.random.default_rng(rng_seed),\n",
    "):\n",
    "    slope, bias = params\n",
    "    x = rng.uniform(*x_span, n_data)\n",
    "    y = slope * x + bias + noise * rng.normal(0.0, 1.0, x.shape)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Affichage du dataset\n",
    "\n",
    "Supposons maintenant que le jeu de données nous arrive entre les mains. \n",
    "En affichant le dataset, on s'informe sur le modèle que l'on souhaite implémenter.\n",
    "En l'occurrence, on fait un choix de **représentation** assez restrictif, à savoir une dépendance affine :\n",
    "$$ y \\approx f(x; w, b) = w x + b. $$\n",
    "On pourrait aussi choisir un modèle sous-jacent probabiliste, mais on cherche juste la «meilleure» droite pour le moment, i.e. les «meilleurs» coefficients $(w,b)$.\n",
    "Il s'agit d'un problème de **régression linéaire**.\n",
    "\n",
    "*Remarque fondamentale :* La relation entre $x$ et $y$ n’est pas linéaire, mais affine. L'appellation «régression linéaire» provient en fait de la dépendance linéaire de $y$ par rapport aux paramètres $(w, b)$. Contrairement aux apparences, les modèles suivants sont bien des modèles linéaires :\n",
    "1. $y^{(k)} = w g(x^{(k)}) + b + \\varepsilon_k$ ;\n",
    "2. Modèle de Cobb-Douglas: $y^{(k)} = b \\bigl( x^{(k)} \\bigr)^w \\varepsilon_k$, car $\\ln(y^{(k)}) = w \\ln(x^{(k)}) + \\tilde{b} + \\ln(\\varepsilon_k)$ avec $\\tilde{b} = \\ln(b)$ ;\n",
    "3. Modèle logistique : $y^{(k)} = \\frac{\\exp( w x^{(k)} + b + \\varepsilon_k)}{1 + \\exp( w x^{(k)} + b + \\varepsilon_k)}$, car $\\ln\\left(\\frac{y^{(k)}}{1 − y^{(k)}}\\right) = w x^{(k)} + b + \\varepsilon_k$.\n",
    "\n",
    "Avec la structure [`Polynomial`](https://numpy.org/doc/stable/reference/routines.polynomials.html) de NumPy, calculer la droite de régression linéaire pour le jeu de données généré. Une droite affine est un polynôme de degré 1. Afficher les coefficients de cette droite et la tracer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data, y_data = get_dataset()\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_scatter(x=x_data, y=y_data, mode=\"markers\", name=\"données\")\n",
    "\n",
    "#TODO utiliser Polynomial.fit pour calculer une régression, et l'afficher\n",
    "affine_fit = ...\n",
    "\n",
    "# affichage des coefficients\n",
    "slope_data, bias_data = affine_fit.deriv()(0), affine_fit(0)\n",
    "print(f\"y = {slope_data} * x + {bias_data}\")\n",
    "\n",
    "fig.update_layout(\n",
    "    width=700,\n",
    "    height=400,\n",
    "    margin=dict(l=20, r=20, b=20, t=20),\n",
    "    xaxis_title=\"x\",\n",
    "    yaxis_title=\"y\",\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Définition du problème inverse\n",
    "\n",
    "Comment déterminer la pente et le biais ? \n",
    "Vous avez déjà fait des régressions linéaires, peut-être avec `polyfit`, peut-être avec d'autres méthodes.\n",
    "Toutes ces méthodes sont basées sur un problème d'optimisation, appelé la **méthode des moindres carrés**, où on cherche $w, b$ qui minimisent la fonctionnelle \n",
    "\n",
    "$$ {\\rm MSE}(w, b) := \\frac{1}{N} \\sum_{k = 1}^N \\bigl( w x^{(k)} + b - y^{(k)} \\bigr)^2 $$\n",
    "\n",
    "L'acronyme ${\\rm MSE}$ signifie *mean squared error*. Affichons cette fonction en fonction des paramètres $(w, b)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# valeurs de pente et de biais \n",
    "w = np.linspace(-2, 10, 50)\n",
    "b = np.linspace(-12, 12, 50)\n",
    "\n",
    "#TODO calcul de la MSE sous forme de matrice, on a w en colonne et b en ligne\n",
    "loss = ...\n",
    "\n",
    "# affichage du graphe\n",
    "fig = go.Figure()\n",
    "fig.add_surface(x=w, y=b, z=loss, showscale=False)\n",
    "fig.update_layout(\n",
    "    autosize=False,\n",
    "    width=400,\n",
    "    height=380,\n",
    "    margin=dict(l=20, r=20, b=20, t=20),\n",
    "    scene=dict(xaxis_title=\"pente\", yaxis_title=\"biais\", zaxis_title=\"erreur\"),\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 2 - Introduction aux tenseurs\n",
    "\n",
    "Dans la cellule au-dessus, `w`, `b`, et `loss` sont des **tenseurs**. \n",
    "Dans ce contexte informatique, ce terme technique indique simplement des tableaux de nombres structurés, une sorte de généralisation des matrices et des vecteurs. \n",
    "Ainsi, `w` et `b` sont des tenseurs d'ordre 1 (i.e. des vecteurs) tandi que `loss` est un tensor d'ordre 2 (i.e. une matrice).\n",
    "Parfois, on remplace le terme «ordre» par «rang» ou «dimension».\n",
    "\n",
    "Avec NumPy, la structure s'appelle [`ndarray`](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html) et avec PyTorch, il s'agit de [`Tensor`](https://pytorch.org/docs/stable/tensors.html).\n",
    "\n",
    "*Remarque :* Dans la communauté physique & mathématique, le concept de tenseur est différent, mais cela ne nous concerne pas ici."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Le caractère structuré des données\n",
    "\n",
    "Dans la cellule suivante, quelles sont les listes de listes qui peuvent être converties en tenseurs ? Vérifier votre raisonnement grâce à la fonction de conversion `array` de NumPy. Quel est l'ordre de ces tenseurs ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 = [[1,2], [3,4,5]]\n",
    "list2 = [[1,2,3], [4,5,6]]\n",
    "\n",
    "#TODO conversion en tenseur si possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Les opérations *componentwise*\n",
    "\n",
    "La différence majeure avec les matrices et les vecteurs est la structure algébrique : par défaut, les additions, produits, etc sont appliqués terme à terme.\n",
    "De même pour les fonctions, comme indiqué ci-dessous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-1, 1, 500)\n",
    "\n",
    "y1 = np.sin(x)\n",
    "\n",
    "y2 = np.empty_like(x) # initie un tenseur avec les mêmes dimensions que x\n",
    "#TODO remplir le vecteur y2 avec une boucle et comparer les résultats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Le *broadcasting*\n",
    "\n",
    "En utilisant la syntaxe `v = u[:, None]` on converti un tenseur de taille $N_u$ en un tenseur de taille $N_u \\times 1$. \n",
    "\n",
    "Grâce à cela, on peut calculer `loss` sans boucle dans l'exemple de code ci-dessous. \n",
    "Attention, cette version a un coût mémoire *beaucoup* plus élevé ($N_w \\times N_b \\times N_x$ vs $N_w \\times N_b + N_x$).\n",
    "Cela évite de faire des boucles, qui sont notoirement longues en Python, mais est parfois quand même plus long du fait de ce surcoût mémoire.\n",
    "\n",
    "**Remarque :** Pour éviter ce surcoût et éviter d'écrire une boucle, on pourrait par exemple utiliser la structure `LazyTensor` du module [`KeOps`](https://www.kernel-operations.io/keops/index.html), mais cela dépasse largement le cadre du cours.\n",
    "\n",
    "Avec la méthode `shape` (e.g. `u.shape`), étudier la taille de tous les tenseurs impliqués dans ce calcul : `w[None, :, None]`, `x[None, None, :]`, leur produit, `b[:, None, None]`, sa somme avec les autres, etc. \n",
    "Les changements de taille sont dûs à des règles de [*broadcasting*](https://numpy.org/doc/stable/user/basics.broadcasting.html) qui deviendront naturelles à force de manipuler ces objets. \n",
    "Le plus important pour éviter les erreurs est de s'assurer que **tous les tenseurs sont du même ordre**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err = w[None, :, None] * x[None, None, :] + b[:, None, None] - y[None, None, :]\n",
    "# on prend la moyenne seulement par rapport à la dimension 2, \n",
    "# i.e. loss[i,j] est la moyenne par rapport à k des err[i,j,k]\n",
    "loss = np.mean(err**2, axis=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Des questions ?\n",
    "\n",
    "Pour avoir plus d'exemples, vous pouvez regarder cette [vidéo de deepmath](https://www.youtube.com/watch?v=XoB6xPMZ8y8). Sinon, passez au TP suivant !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuralnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
