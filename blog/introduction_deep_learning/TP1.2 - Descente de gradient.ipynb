{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet 1, TP2 - Descente de gradient d√©terministe\n",
    "\n",
    "Pour l'exemple de r√©gression lin√©aire du TP pr√©c√©dent, les m√©thodes de r√©solution directe que vous avez d√©j√† utilis√©es calculent une matrice $2 \\times 2$ et l'inversent. \n",
    "Dans la suite, on va faire comme si cette matrice et cette r√©solution de syst√®me √©taient trop co√ªteuses pour notre petit ordinateur. \n",
    "√Ä la place, on va consid√©rer une m√©thode it√©rative qui ne consid√®re pas de matrice : **la descente de gradient**.\n",
    "\n",
    "*Remarque¬†:* Si vous souhaitez lire des articles, le domaine de l'optimisation d√©note g√©n√©ralement $x$ la variable selon laquelle on optimise, et $x \\mapsto f(x)$ la fonction co√ªt. En apprentissage automatique, $x$ d√©note g√©n√©ralement les *inputs* et $f$ est le mod√®le qu'on √©value, de param√®tres d√©not√©s $\\theta$. On minimise une fonction co√ªt $\\theta \\mapsto \\mathcal{L}(\\theta)$ par rapport aux param√®tres $\\theta$. Il faut faire attention √† ne pas m√©langer les notations en fonction de ce que l'on lit. Ici on va utiliser le formalisme de l'apprentissage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.polynomial.polynomial import Polynomial\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import torch\n",
    "\n",
    "rng_seed = sum(ord(c) ** 2 for c in \"R5.A.12-ModMath\")\n",
    "torch.manual_seed(rng_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### La fonction √† minimiser\n",
    "\n",
    "Ci-dessous, on d√©finit et on affiche la fonction $\\theta \\mapsto \\mathcal{L}(\\theta)$ que l'on cherche √† minimiser, d√©finie par\n",
    "$$ \\mathcal{L}(\\theta) = P(\\theta) / (1 + \\theta^2) , $$\n",
    "avec $P$ un polyn√¥me de degr√© 4 bien choisi. \n",
    "Elle admet deux minima locaux, dont un minimum global. \n",
    "En ces points, la fonction, que l'on appelle *loss* vaut `-25.33866607` ou `-8.34234203`. \n",
    "L'objectif de cette partie est de trouver num√©riquement ces minima et les *loss* associ√©es."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D√©finition et affichage de la fonction √† minimiser\n",
    "\n",
    "# coefficients polynomiaux bien choisis pour l'exemple üòá\n",
    "example_coeffs = (\n",
    "    16.8350101057134,\n",
    "    10.621467774429892,\n",
    "    -45.29852223580801,\n",
    "    3.380171476274114,\n",
    "    6.068075889020002,\n",
    ")\n",
    "example_poly = Polynomial(example_coeffs)\n",
    "\n",
    "\n",
    "def example_loss_fun(param):\n",
    "    return example_poly(param) / (1.0 + param**2)\n",
    "\n",
    "\n",
    "param_min, param_max = -4.5, 3.5\n",
    "param_vals_continuous = np.linspace(param_min, param_max, 500)\n",
    "loss_vals_continuous = example_loss_fun(param_vals_continuous)\n",
    "\n",
    "trace_continuous = go.Scatter(\n",
    "    x=param_vals_continuous, y=loss_vals_continuous, name=\"erreur\"\n",
    ")\n",
    "fig_continuous = go.Figure(\n",
    "    data=[trace_continuous],\n",
    "    layout=dict(\n",
    "        width=500,\n",
    "        height=250,\n",
    "        margin=dict(l=20, r=20, b=20, t=20),\n",
    "        xaxis_title=\"param\",\n",
    "        yaxis_title=\"erreur\",\n",
    "    ),\n",
    ")\n",
    "fig_continuous.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approximation lin√©aire\n",
    "\n",
    "Pour diff√©rents points $\\theta_0$, on trace la droite tangente d'√©quation\n",
    "$$ y = \\ell(\\theta; \\theta_0) := \\nabla\\mathcal{L}(\\theta_0) (\\theta - \\theta_0) + \\mathcal{L}(\\theta) . $$\n",
    "o√π $\\nabla\\mathcal{L}$ d√©note la d√©riv√©e de $\\mathcal{L}$. Pour les fonctions √† plusieurs variables, on appellera cela le gradient, que l'on d√©finira au prochain TP.\n",
    "\n",
    "Zoomer autour de ces points pour v√©rifier qu'on a bien $\\ell(\\theta, \\theta_0) \\approx \\mathcal{L}(\\theta)$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D√©finition de la fonction d√©riv√©e et affichage de l'approximation lin√©aire en quelques points\n",
    "\n",
    "example_poly_deriv = example_poly.deriv()\n",
    "\n",
    "\n",
    "def example_loss_fun_deriv(param):\n",
    "    poly_vals = example_poly(param)\n",
    "    dpoly_vals = example_poly_deriv(param)\n",
    "    param_sq_p1 = 1.0 + param**2\n",
    "    return (dpoly_vals * param_sq_p1 - 2.0 * param * poly_vals) / param_sq_p1**2\n",
    "\n",
    "\n",
    "fig = go.Figure(fig_continuous)\n",
    "\n",
    "param_vals = np.array([-4.0, -2.5, -0.5, 1.0, 2.5])\n",
    "loss_vals = example_loss_fun(param_vals)\n",
    "dloss_vals = example_loss_fun_deriv(param_vals)\n",
    "\n",
    "fig.add_scatter(\n",
    "    x=param_vals,\n",
    "    y=loss_vals,\n",
    "    mode=\"markers+text\",\n",
    "    marker=dict(color=\"red\"),\n",
    "    text=[\"(a)\", \"(b)\", \"(c)\", \"(d)\", \"(e)\"],\n",
    "    textposition=\"top center\",\n",
    ")\n",
    "\n",
    "for theta_0, loss_0, dloss_0 in zip(param_vals, loss_vals, dloss_vals):\n",
    "    theta_span = theta_0 + 0.7 * np.array([-1.0, 1.0])\n",
    "    affine_span = dloss_0 * (theta_span - theta_0) + loss_0\n",
    "    fig.add_scatter(\n",
    "        x=theta_span, y=affine_span, mode=\"lines\", line=dict(color=\"red\", dash=\"dot\")\n",
    "    )\n",
    "\n",
    "fig.update_yaxes()\n",
    "y_min_tmp, y_max_tmp = loss_vals_continuous.min(), loss_vals_continuous.max()\n",
    "y_min = y_min_tmp - 0.1 * (y_max_tmp - y_min_tmp)\n",
    "y_max = y_max_tmp + 0.1 * (y_max_tmp - y_min_tmp)\n",
    "fig.update_layout(\n",
    "    xaxis=dict(title=\"param\", range=[param_min, param_max]),\n",
    "    yaxis=dict(title=\"erreur\", range=[y_min, y_max]),\n",
    "    showlegend=False,\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 1 - L'algorithme de descente de gradient\n",
    "\n",
    "√Ä partir d'un param√®tre initial $\\theta_0$, on cherche √† se rapprocher du puits le plus proche. \n",
    "Pour cela, on d√©finit un nouveau point $\\theta_1$ qui va d√©pendre notamment du signe de la d√©riv√©e : \n",
    "- Si $\\nabla\\mathcal{L}(\\theta_0)$ est strictement positive (comme au point $(c)$ ci-dessus), alors la fonction est croissante, et on veut choisir $\\theta_1 < \\theta_0$. \n",
    "- si $\\nabla\\mathcal{L}(\\theta_0) < 0$, alors $\\theta_1 > \\theta_0$. Enfin, si $\\mathcal{L}(\\theta_0) = 0$. \n",
    "- si $\\nabla\\mathcal{L}(\\theta_0) = 0$, on supposera qu'on est sur un minimum.\n",
    "\n",
    "Cela revient √† ¬´glisser¬ª le long de la tangente, d√©finir un nouveau param√®tre $\\theta_1$ correspondant √† une *loss* plus faible, puis recommencer pour d√©finir un $\\theta_2$, etc.\n",
    "On d√©finit ainsi une suite $(\\theta_t)_{t \\geq 0}$ o√π l'erreur s'am√©liore petit √† petit.\n",
    "Ci-dessous, on fournit une fonction `plot_error_evolution` qui permet d'afficher la suite et l'√©volution de l'erreur en fonction de $t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_error_evolution(theta_t, fig=None, min_val=None):\n",
    "    alpha = np.linspace(0.0, 1.0, len(theta_t))\n",
    "    loss_t = example_loss_fun(theta_t)\n",
    "\n",
    "    if fig is None:\n",
    "        fig = make_subplots(rows=1, cols=2)\n",
    "        fig.add_trace(trace_continuous, row=1, col=1)\n",
    "        fig.update_layout(\n",
    "            width=800, height=250, margin=dict(l=20, r=20, b=20, t=20), showlegend=False\n",
    "        )\n",
    "        fig.update_yaxes(type=\"log\", row=1, col=2)\n",
    "\n",
    "    params = dict(mode=\"markers\", marker=dict(color=alpha), row=1, col=1)\n",
    "    fig.add_scatter(x=theta_t, y=loss_t, name=\"(a)\", **params)\n",
    "\n",
    "    if min_val is None:\n",
    "        min_val = loss_t.min() + 1e-8\n",
    "    fig.add_scatter(y=loss_t - min_val, row=1, col=2)\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Descente brutale vers le puits\n",
    "\n",
    "L'id√©e la plus simple est de faire un pas dans la bonne direction, \n",
    "$$ \\theta_{t+1} = \\theta_t - \\gamma\\ {\\rm signe}\\bigl( \\nabla\\mathcal{L}(\\theta_t) \\bigr) . $$\n",
    "Le param√®tre $\\gamma$ s'appelle le *learning rate*. \n",
    "On obtient une approximation $\\mathcal{L}(\\theta_{t+1}) \\approx \\mathcal{L}(\\theta_t) - \\gamma | \\nabla\\mathcal{L}(\\theta_t) |$, ce qui indique que l'erreur devrait d√©croitre.\n",
    "\n",
    "Pour $\\theta_0 = -3.8$, calculer les points $\\theta_t$ pour $t$ allant de $1$ √† $10$ avec $\\gamma = 0.5$. √Ä l'aide de la fonction `plot_error_evolution`, afficher ces points ainsi que l'√©volution de la diff√©rence entre $\\mathcal{L}(\\theta_t)$ et le minimum le plus proche en fonction de $t$. Est-ce qu'on atteint bien le minimum le plus proche ? Qu'en est-il si on d√©marre avec $\\theta_0 = 3.3$ ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.5\n",
    "num_iters = 10\n",
    "theta_t = np.repeat([[-3.8, 3.3]], num_iters, axis=0)\n",
    "for t in range(1, num_iters):\n",
    "    #TODO appliquer les it√©rations\n",
    "    # theta_t[t] = ...\n",
    "\n",
    "fig = plot_error_evolution(theta_t[:, 0], min_val=-25.33866607)\n",
    "fig = plot_error_evolution(theta_t[:, 1], fig=fig, min_val=-8.34234203)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. La descente de gradient classique\n",
    "\n",
    "Plut√¥t que de faire √©voluer le param√®tre avec un pas fixe, on choisit le pas en fonction de la proximit√© au puits :\n",
    "$$ \\theta_{t+1} = \\theta_t - \\gamma\\ \\nabla\\mathcal{L}(\\theta_t) , $$\n",
    "ce qui correspond √† une approximation $\\mathcal{L}(\\theta_{t+1}) \\approx \\mathcal{L}(\\theta_t) - \\gamma \\bigl( \\mathcal{L}(\\theta_t) \\bigr)^2$.\n",
    "\n",
    "Pour $\\theta_0 = -3.8$, calculer les points $\\theta_t$ pour $t$ allant de $1$ √† $10$ avec $\\gamma = 0.5$. √Ä l'aide de la fonction `plot_error_evolution`, afficher ces points ainsi que l'√©volution de la diff√©rence entre $\\mathcal{L}(\\theta_t)$ et le minimum le plus proche en fonction de $t$. Est-ce qu'on atteint bien le minimum le plus proche ? Qu'en est-il si on d√©marre avec $\\theta_0 = 3.3$ ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO appliquer l'algorithme de descente de gradient et afficher les r√©sultats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. L'effet du *learning rate*\n",
    "\n",
    "Effectuer la m√™me √©tude de la descente de gradient avec $\\gamma = 10^-3$ et $120$ it√©rations. Commenter.\n",
    "\n",
    "*Remarque :* Le *learning rate* $\\gamma$ est un **hyperparam√®tre**, c'est-√†-dire un param√®tre qu'on n'apprend pas. Il est g√©n√©ralement entre $10^{-2}$ et $10^{-5}$ en fonction des applications. Voici une br√®ve [vid√©o d'explication](https://www.youtube.com/watch?v=TwJ8aSZoh2U) (en anglais) sur le comportement de l'apprentissage en fonction de cet hyperparam√®tre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 2 - Diff√©rentiation automatique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Le graphe de computation\n",
    "\n",
    "Un int√©r√™t majeur de PyTorch est que les tenseurs ont une ¬´m√©moire¬ª des diff√©rentes op√©rations dont ils sont issus √† travers un *graphe de calcul*, ce qui permet de calculer automatiquement la d√©riv√©e. Il y a pour √ßa deux approches, √† partir d'une assignation `y = f(x0, x1)` :\n",
    "1. la fonction `torch.autograd.grad(y, (x0, x1))` qui renvoie un tuple `(dy/dx0, dy/dx1)` ;\n",
    "2. la m√©thode `torch.Tensor.backward()` qui ajoute `dy/dx0` √† `x0.grad`, et de m√™me pour `x1`.\n",
    "\n",
    "Pour appliquer ces approches, il faut **au pr√©alable** indiquer que les tenseurs `x0` et `x1` sont diff√©rentiables, soit avec l'argument `requires_grad=True` lors de la construction, soit avec la m√©thode `x0.requires_grad_()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deux constructions √©quivalentes\n",
    "x0 = torch.randn((), requires_grad=True)\n",
    "x1 = torch.randn(()).requires_grad_()\n",
    "\n",
    "y0 = x0 * x1\n",
    "grad0, grad1 = torch.autograd.grad(y0, (x0, x1))\n",
    "y1 = x0 * x1  # m√™me calcul\n",
    "y1.backward()\n",
    "\n",
    "# comparaison des m√©thodes\n",
    "x0.grad - grad0, x1.grad - grad1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Quelques avertissements\n",
    "\n",
    "*Avertissements :* \n",
    "1. Par d√©faut, le graphe de calcul est supprim√© apr√®s l'appel de `grad` ou de `backward`. On ne peut pas effectuer cette op√©ration deux fois sur le m√™me tenseur ;\n",
    "2. Avec `grad(y, x0)`, la sortie est un 1-tuple `(dy/dx0,)` ;\n",
    "3. Avec `backward`, on **ajoute** le gradient √† `x.grad`, il faut penser √† le remettre √† z√©ro lorsque n√©cessaire ;\n",
    "4. Si on ne fait pas attention et qu'on encha√Æne trop d'op√©rations avec un tenseur diff√©rentiable, l'impact m√©moire du graphe de calcul risque d'exploser.\n",
    "\n",
    "On illustre le troisi√®me point ci-dessous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deux constructions √©quivalentes\n",
    "x0 = torch.randn((), requires_grad=True)\n",
    "x1 = torch.randn(()).requires_grad_()\n",
    "\n",
    "y = x0 * x1\n",
    "z = x0 + x1\n",
    "\n",
    "y.backward()\n",
    "z.backward()\n",
    "somme_grads = x1.item() + 1.0  # la m√©thode item fait la conversion tenseur -> float\n",
    "\n",
    "print(x0.grad.item() - somme_grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Modification d'un tenseur diff√©rentiable\n",
    "\n",
    "Pour modifier un tenseur diff√©rentiable, il faut faire l'op√©ration dans un environnement `torch.no_grad()`.\n",
    "\n",
    "*Remarque :* Avec TensorFlow, la philosophie est oppos√©e : toutes les op√©rations sont `no_grad` par d√©faut et il faut sp√©cifier lorsque l'on souhaite grapher les calculs.\n",
    "\n",
    "Observer cette contrainte dans la cellule suivante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_update(x):\n",
    "    x -= 0.1\n",
    "    return x\n",
    "\n",
    "def no_grad_update(x):\n",
    "    with torch.no_grad():\n",
    "        x -= 0.1\n",
    "    return x\n",
    "\n",
    "x = torch.randn((), requires_grad=True)\n",
    "\n",
    "#TODO comparer les deux fonctions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Application au probl√®me d'optimisation\n",
    "\n",
    "En utilisant `torch.autograd.grad`, mettre en place l'algorithme de descente de gradient avec $\\theta_0 = -3.8$.\n",
    "Penser √† utiliser `Tensor.item()` pour sauvegarder les valeurs de $\\theta_t$ sans conserver le graphe de calcul.\n",
    "\n",
    "Au prochain TP, nous utiliserons la structure `Optimizer` de PyTorch qui permet d'utiliser `backwards` sans avoir de souci avec l'accumulation des gradients, et sans avoir √† se soucier des environnements `no_grad`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 3 - Pour aller plus loin\n",
    "\n",
    "On pr√©sente ici une am√©lioration possible √† la descente de gradient, et des liens avec le contexte de l'apprentissage automatique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Descente avec moment\n",
    "\n",
    "Une am√©lioration possible de la descente de gradient est l'ajout d'un moment, c'est-√†-dire une inertie √† l'amplitude de descente. \n",
    "L'algorithme implique maintenant une nouvelle variable $g_t$ repr√©sentant le gradient intertiel, et s'√©crit\n",
    "$$ g_{t+1} = \\nabla\\mathcal{L}(\\theta_t) + \\mu g_t, \\qquad \\theta_{t+1} = \\theta_t - g_{t+1} . $$\n",
    "pour une param√®tre $\\mu$ appel√© le *momentum*.\n",
    "Voici une [vid√©o d'explication](https://www.youtube.com/watch?v=r-rYz_PEWC8) (en anglais). \n",
    "Cela permet parfois de sortir de minima locaux.\n",
    "\n",
    "*Remarque :* Il existe encore d'autres m√©thodes, qui choisisse par exemple $\\gamma$ en fonction du comportement de la fonction, ou qui se basent sur des analyses plus fine comme avec la m√©thode de Newton ou les m√©thodes quasi-Newton. Pour le deep learning, on observera au prochain TP le comportement de la m√©thode [Adam](https://arxiv.org/abs/1412.6980).\n",
    "\n",
    "Mettre en place cette m√©thode avec $\\gamma = 10^{-2}$, $\\mu = 0.95$ et $\\theta_0 = 3.3$, sur $200$ it√©rations. Afficher les r√©sultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. L'int√©r√™t des *checkpoints*\n",
    "\n",
    "Recommencer avec $\\gamma = 10^{-2}$, $\\mu = 0.97$ et $\\theta_0 = -3.8$, sur $200$ it√©rations.\n",
    "\n",
    "On voit que l'erreur diminue, puis augmente fortement. Lors d'un apprentissage, il est int√©ressant de sauvegarder le meilleur r√©sultat au cas o√π on sortirait du ¬´bon¬ª puits. On peut alors recommencer ponctuellement depuis ce point avec un *learning rate* plus faible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Entra√Æner avec divers param√®tres initiaux\n",
    "\n",
    "On voit dans l'exemple pr√©c√©dent que selon o√π le param√®tre initial se situe, la descente de gradient ne converge pas vers le m√™me minimum. \n",
    "En l'occurrence, il y a deux puits, et on converge vers l'un ou l'autre en fonction qu'on choisisse $\\theta_0$ √† gauche ou √† droite du maximum local central.\n",
    "Ci-dessous, un exemple de fonction bien plus pathologique (fonction de Whitley, issue de [ce rapport, Sec. A.7](https://www.uni-goettingen.de/de/document/download/9aa76b19d6bc767fb1f9733b21854cb5.pdf/Bachelorarbeit_Brachem_Carsten.pdf)), plus similaire √† ce qui appara√Æt en apprentissage.\n",
    "On voit bien qu'il peut y avoir de tr√®s diverses valeurs de *loss* finale. \n",
    "Ainsi, en *deep learning*, il faut parfois entrainer plusieurs mod√®les initialis√©s diff√©remment, et choisir le meilleur.\n",
    "\n",
    "Utiliser `torch.randn` pour initialiser plusieurs $\\theta_0$, et effectuer plusieurs descentes de gradient en m√™me temps. S√©lectionner ensuite le meilleur param√®tre. On pourra utiliser une descente de gradient standard ou une descente plus √©labor√©e."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def whitney_loss_fun(x):\n",
    "    xs = (-1.6681, 0.0369, -2.0580, -1.4312, -1.9171, -0.5520)\n",
    "    x_m1 = (1.0 - x) ** 2\n",
    "    tot = x_m1**2 / 4000.0 - torch.cos(x_m1**2) + 1.0\n",
    "    for xi in xs:\n",
    "        ener_i = ((xi - x) ** 2 + x_m1) ** 2\n",
    "        tot += ener_i / 4000.0 - torch.cos(ener_i) + 1.0\n",
    "    return tot\n",
    "\n",
    "\n",
    "x = torch.linspace(-1.0, 1.0, 2000)\n",
    "y = whitney_loss_fun(x)\n",
    "go.Figure(\n",
    "    data=[go.Scatter(x=x.numpy(), y=y.numpy())],\n",
    "    layout=dict(\n",
    "        width=800,\n",
    "        height=250,\n",
    "        margin=dict(l=20, r=20, b=20, t=20),\n",
    "        xaxis_title=\"param\",\n",
    "        yaxis_title=\"erreur\",\n",
    "    ),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuralnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
