{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet 1, TP3 - Optimisation avec PyTorch\n",
    "\n",
    "\n",
    "En partie 1, on définit les données et une classe `torch.nn.Module` qui correspond à notre modèle.\n",
    "En partie 2, on met en place une boucle d'apprentissage pour effectuer la descente de gradient sur ses paramètres, et on vérifie la cohérence du code.\n",
    "En partie 3, on met en place la boucle d'apprentissage qui utilise la classe `torch.optim.Optimizer`. \n",
    "C'est la structure de code standard en PyTorch pour l'apprentissage dont on se servira par la suite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quelques imports utiles\n",
    "import numpy as np\n",
    "from numpy.polynomial.polynomial import Polynomial\n",
    "\n",
    "import plotly\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "DEFAULT_SEED = sum(ord(c) ** 2 for c in \"R5.A.12-ModMath\")\n",
    "torch.manual_seed(DEFAULT_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 1 - Un modèle en syntaxe PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Définition et génération du dataset\n",
    "\n",
    "Les entrées $(x^{(k)})_k$ sont dans un tenseur de taille $N \\times 1$, et les sorties $(y^{(k)})_k$ dans un tenseur de taille $N \\times 1$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(\n",
    "    params=(5.0, -2.0),\n",
    "    x_span=(-3.0, 3.0),\n",
    "    n_data=100,\n",
    "    noise_amplitude=6.5,\n",
    "    rng_seed=DEFAULT_SEED,\n",
    "):\n",
    "    slope, bias = params\n",
    "    rng = torch.Generator().manual_seed(rng_seed)\n",
    "    x = torch.empty(n_data, 1).uniform_(*x_span, generator=rng)\n",
    "    noise = torch.empty(n_data, 1).normal_(0.0, noise_amplitude, generator=rng)\n",
    "    y = slope * x + bias + noise\n",
    "    return x, y\n",
    "\n",
    "\n",
    "x_data, y_data = get_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Définir le modèle\n",
    "\n",
    "On définit un modèle de régression linéaire à deux paramètres, $\\theta = (w, b)$ avec \n",
    "$$ f_\\theta(x) = w x + b , \\qquad w \\in \\mathbb{R},\\ b \\in \\mathbb{R} . $$\n",
    "L'implémentation utilise la structure `torch.nn.Module` qui permet d'enregistrer des paramètres.\n",
    "Ici on choisit des paramètres initiaux $w = 0$ et $b = 5$, qu'on cherchera à optimiser pour que le modèle colle aux données générées au sens des moindres carrés.\n",
    "Cela change du TP1.1 où l'on calculait à la main la sortie du modèle.\n",
    "\n",
    "On voit que les paramètres sont enregistrés avec la structure `torch.nn.Parameter`. On se servira de cela pour prendre de la hauteur en partie 3.\n",
    "\n",
    "**TODO:**  \n",
    "Compléter la méthode `forward` ci-dessous, qui correspond à la fonction $x \\mapsto f_\\theta(x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self, w=0.0, b=5.0):\n",
    "        super().__init__()\n",
    "        self.slope = nn.Parameter(torch.tensor(w))\n",
    "        self.bias = nn.Parameter(torch.tensor(b))\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass #TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Calcul de l'erreur moyenne\n",
    "\n",
    "Dans le TP 1.1, on a défini la fonction `mean_squared_error`. \n",
    "Ici, on va plutôt utiliser la classe `torch.nn.MSELoss` de PyTorch, qui permet d'obtenir une fonction qui calcule la distance moyenne entre la prédiction du modèle $y_{\\rm pred}^{(k)} = f_\\theta(x_{\\rm data}^{(k)})$ et la sortie connue $y_{\\rm data}^{(k)}$,\n",
    "$$ {\\rm MSE}(y_{\\rm pred}, y_{\\rm data}) = \\frac{1}{N} \\sum_{k=1}^N \\bigl( y_{\\rm pred}^{(k)} - y_{\\rm data}^{(k)} \\bigr)^2 . $$\n",
    "\n",
    "**TODO:**  \n",
    "Compléter la boucle ci-dessous qui calcule la matrice `loss` pour l'affichage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# valeurs de pente et de biais\n",
    "w = np.linspace(-10, 10, 40)\n",
    "b = np.linspace(-10, 10, 30)\n",
    "\n",
    "loss_fun = nn.MSELoss()  # instanciation de la MSE\n",
    "\n",
    "loss = np.zeros((len(b), len(w)))\n",
    "for i in range(len(b)):\n",
    "    for j in range(len(w)):\n",
    "        #TODO construire un modèle avec une pente w[j] et un biais b[j]\n",
    "        #TODO calculer la prédiction effectuée par ce modèle\n",
    "        #TODO remplir la matrice `loss` avec l'erreur\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Initialisation de graphes pour affichages futurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_plot, y_plot = x_data.numpy()[:, 0], y_data.numpy()[:, 0]\n",
    "trace_data = go.Scatter(x=x_plot, y=y_plot, mode=\"markers\")\n",
    "\n",
    "contour_params = dict(\n",
    "    contours_coloring=\"lines\",\n",
    "    colorscale=\"Greys_r\",\n",
    "    showscale=False,\n",
    "    contours=dict(showlabels=True, labelfont=dict(size=10)),\n",
    "    ncontours=40,\n",
    "    line=dict(smoothing=1.3)\n",
    ")\n",
    "trace_loss = go.Contour(x=w, y=b, z=loss, **contour_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 2 - Calcul automatique des gradients, itérations manuelles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Itérations de descente de gradient\n",
    "\n",
    "Pour cette partie, le code prend la forme suivante :\n",
    "\n",
    "1. Construire le modèle de `LinearRegression` avec les paramètres par défaut.\n",
    "2. Instantier la fonction de *loss* des moindres carrés.\n",
    "3. Définir le learning rate $\\gamma = 0.1$.\n",
    "4. Effectuer des itérations de la **boucle d'apprentissage** standard :\n",
    "    1. Calculer les prédictions du modèle et les comparer aux données grâce à la fonction de loss.\n",
    "    2. Calculer les gradients de la *loss* par rapport à chaque paramètre avec `torch.autograd.grad`.\n",
    "    3. Mettre à jour les paramètres grâce aux gradients calculés.\n",
    "\n",
    "\n",
    "**TODO:**  \n",
    "Remplir la cellule suivante pour suivre ces étapes. \n",
    "Penser à sauvegarder les valeurs des paramètres à chaque itération pour les afficher ensuite.\n",
    "Pour copier un tenseur sans conserver son graphe de calcul, il faut l'en «détacher» avec `.detach()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Évolution des paramètres\n",
    "\n",
    "**TODO:**  \n",
    "Compléter le code d'affichage suivant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# création de la figure\n",
    "fig = plotly.subplots.make_subplots(\n",
    "    rows=1,\n",
    "    cols=2,\n",
    "    column_widths=[0.41, 0.59],\n",
    "    column_titles=[\n",
    "        \"Erreur en fonction des paramètres\",\n",
    "        \"Données et régressions linéaires\",\n",
    "    ],\n",
    ")\n",
    "fig.update_layout(\n",
    "    width=800, height=350, margin=dict(l=20, r=20, b=20, t=20), showlegend=False\n",
    ")\n",
    "\n",
    "# topographie de l'erreur dans l'espace des paramètres\n",
    "fig.add_trace(trace_loss, row=1, col=1)\n",
    "fig.update_xaxes(title=\"w\", row=1, col=1)\n",
    "fig.update_yaxes(title=\"b\", row=1, col=1)\n",
    "\n",
    "fig.add_trace(trace_data, row=1, col=2)\n",
    "fig.update_xaxes(title=\"x\", row=1, col=2)\n",
    "fig.update_yaxes(title=\"y\", row=1, col=2)\n",
    "\n",
    "## affichage des points sur la surface\n",
    "all_colors = plotly.colors.sample_colorscale(\n",
    "    plotly.colors.sequential.Plasma, samplepoints=np.linspace(0.0, 1.0, len(all_w))\n",
    ")\n",
    "marker_params = dict(size=10, color=all_colors)\n",
    "scatter_params = dict(marker=marker_params, mode=\"markers\", row=1, col=1)\n",
    "#TODO afficher les valeurs de pente et de biais dans le plan au fil des itérations\n",
    "\n",
    "#TODO affichage des droites (i.e. du modèle) pour chaque valeur des paramètres\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 3 - Utilisation de la structure `Optimizer`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. La structure d'apprentissage PyTorch\n",
    "\n",
    "On se base sur [ce tutoriel PyTorch](https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html) pour obtenir la structure du code, même si notre modèle est différent.\n",
    "\n",
    "1. Construire le modèle de `LinearRegression` avec les paramètres par défaut.\n",
    "2. Instantier la fonction de *loss* des moindres carrés.\n",
    "3. Instantier l'algorithme d'optimisation avec `torch.optim.SGD` et un learning rate $\\gamma = 0.1$.\n",
    "4. Effectuer des itérations de la **boucle d'apprentissage** standard :\n",
    "    1. Réinitialiser les gradients des paramètres du modèle avec `.zero_grad()`. Par défaut, les gradients s'additionnent ; pour éviter un double comptage, nous les remettons explicitement à zéro à chaque itération.\n",
    "    2. Calculer les prédictions du modèle et les comparer aux données grâce à la fonction de loss.\n",
    "    3. Rétropropager la perte de prédiction avec la méthode `.backward()` -- PyTorch dépose les gradients de la perte par rapport à chaque paramètre.\n",
    "    4. Appliquer l'optimisateur avec la méthode `.step()` pour ajuster les paramètres en fonction des gradients collectés lors de la rétropropagation.\n",
    "\n",
    "**TODO:**  \n",
    "Remplir la cellule suivante pour suivre ces étapes. Penser à sauvegarder les valeurs des paramètres à chaque itération pour les afficher ensuite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Affichage de l'évolution des paramètres\n",
    "\n",
    "**TODO:**  \n",
    "Compléter le code d'affichage suivant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# création de la figure\n",
    "fig = plotly.subplots.make_subplots(\n",
    "    rows=1,\n",
    "    cols=2,\n",
    "    column_widths=[0.41, 0.59],\n",
    "    column_titles=[\n",
    "        \"Erreur en fonction des paramètres\",\n",
    "        \"Données et régressions linéaires\",\n",
    "    ],\n",
    ")\n",
    "fig.update_layout(\n",
    "    width=800, height=350, margin=dict(l=20, r=20, b=20, t=20), showlegend=False\n",
    ")\n",
    "\n",
    "# topographie de l'erreur dans l'espace des paramètres\n",
    "fig.add_trace(trace_loss, row=1, col=1)\n",
    "fig.update_xaxes(title=\"w\", row=1, col=1)\n",
    "fig.update_yaxes(title=\"b\", row=1, col=1)\n",
    "\n",
    "fig.add_trace(trace_data, row=1, col=2)\n",
    "fig.update_xaxes(title=\"x\", row=1, col=2)\n",
    "fig.update_yaxes(title=\"y\", row=1, col=2)\n",
    "\n",
    "## affichage des points sur la surface\n",
    "all_colors = plotly.colors.sample_colorscale(\n",
    "    plotly.colors.sequential.Plasma, samplepoints=np.linspace(0.0, 1.0, len(all_w))\n",
    ")\n",
    "marker_params = dict(size=10, color=all_colors)\n",
    "scatter_params = dict(marker=marker_params, mode=\"markers\", row=1, col=1)\n",
    "#TODO afficher les valeurs de pente et de biais dans le plan au fil des itérations\n",
    "\n",
    "#TODO affichage des droites (i.e. du modèle) pour chaque valeur des paramètres\n",
    "\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuralnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
