{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet 1, TP4 - Apprentissage par lots\n",
    "\n",
    "On continue à suivre [le tutoriel PyTorch](https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html#full-implementation). On remarque notamment que la boucle d'apprentissage ressemble à la notre :\n",
    "\n",
    "```py\n",
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "```\n",
    "\n",
    "Cependant lorsqu'on regarde l'implémentation de `train_loop`, il contient une autre boucle,\n",
    "\n",
    "```py\n",
    "for batch, (X, y) in enumerate(dataloader):\n",
    "```\n",
    "\n",
    "C'est quoi un `dataloader`? Et pourquoi est-ce qu'il y a une boucle dans la boucle ?\n",
    "\n",
    "Précédemment, on a donné fait une descente de gradient en prenant toutes les données en entrée du réseau. \n",
    "En pratique, pour beaucoup d'applications (par exemple des images), cette quantité d'information est bien trop importante pour la mémoire de notre ordinateur, et il faut traiter les données **par lots** (par *batches*).\n",
    "Ainsi, dans le tutoriel, on trouve une ligne `train_dataloader = DataLoader(training_data, batch_size=64)`, qui initialise un itérable qui sert à obtenir un lot après l'autre, en l'occurrence avec des sous-échantillons de taille 64.\n",
    "Un passage complet dans cette boucle interne s'appelle une **époque** (ou *epoch*).\n",
    "\n",
    "Dans ce TP, on va étudier ce que ce comportement change \n",
    "\n",
    "*Remarque :* Le tutoriel contient aussi un `test_loop` et un `test_dataloader`. Ici, on n'a pas besoin de distinguer les jeux de données d'«entraînement» et de «validation», parce que notre **représentation** est très bien choisie. Il n'y a pas de risque d'*overfitting*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quelques imports utiles\n",
    "\n",
    "import numpy as np\n",
    "from numpy.polynomial.polynomial import Polynomial\n",
    "\n",
    "import plotly\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import projet1 as p1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data, y_data = p1.get_dataset()\n",
    "dataset = TensorDataset(x_data, y_data)\n",
    "\n",
    "slope_plot = np.linspace(-10, 10, 40)\n",
    "bias_plot = np.linspace(-10, 10, 30)\n",
    "loss_plot = p1.build_loss(slope_plot, bias_plot, x_data, y_data)\n",
    "\n",
    "data_trace = p1.data_scatter_trace(x_data, y_data)\n",
    "loss_trace = p1.init_contour_trace(slope_plot, bias_plot, loss_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 1 - Introduction aux batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Apprentissage sur une époque\n",
    "\n",
    "**TODO:**  \n",
    "Modifier cette fonction pour sauvegarder les valeurs des paramètres (pente et biais) du modèle après chaque pas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(dataloader, model, loss_fun, optimizer):\n",
    "    for x_batch, y_batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y_pred = model(x_batch)\n",
    "        batch_loss = loss_fun(y_pred, y_batch)\n",
    "\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #TODO gérer la sauvegarde de l'historique des paramètres\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. La boucle d'apprentissage\n",
    "\n",
    "**TODO:**  \n",
    "Construire un *dataloader* avec des lots de la même taille que le nombre de données et appliquer une boucle d'apprentissage sur 20 époques. Vérifier que les résultats correspondent à la boucle d'apprentissage sans *dataloader* du TP précédent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_linreg(\n",
    "    n_epochs=100, batch_size=0, model=None, optim_algo=torch.optim.SGD, optim_params={}\n",
    "):\n",
    "    if model is None:\n",
    "        model = p1.LinearRegression()\n",
    "\n",
    "    dataset = TensorDataset(*p1.get_dataset())\n",
    "    # Notation avec étoile pour déplier le tuple, équivalent à :\n",
    "    # x_data, y_data = p1.get_dataset()\n",
    "    # dataset = TensorDataset(x_data, y_data)\n",
    "    if batch_size == 0:\n",
    "        batch_size = len(dataset)\n",
    "\n",
    "    # TODO définir le dataloader\n",
    "\n",
    "    loss_fun = nn.MSELoss()\n",
    "    optimizer = optim_algo(model.parameters(), **optim_params)\n",
    "\n",
    "    for _ in range(n_epochs):\n",
    "        w_epoch, b_epoch = train_epoch(dataloader, model, loss_fun, optimizer)\n",
    "\n",
    "        # TODO gérer la sauvegarde de l'historique des paramètres dans w_history et b_history\n",
    "\n",
    "    return model, w_history, b_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Des lots de taille maximale\n",
    "\n",
    "**TODO:**  \n",
    "Vérifier qu'avec `batch_size=0`, on retrouve bien les résultats du TP précédent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, all_w, all_b = train_linreg(n_epochs=15, optim_params=dict(lr=1e-1))\n",
    "p1.training_evolution(all_w, all_b, loss_trace=loss_trace, data_trace=data_trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. L'apprentissage par lots\n",
    "\n",
    "Faire tourner des apprentissages avec les paramètres suivants :\n",
    "\n",
    "- `n_epochs=7, batch_size=25, learning_rate=5e-2`\n",
    "- `n_epochs=7, batch_size=5, learning_rate=1e-2`\n",
    "- `n_epochs=15, batch_size=1, learning_rate=1e-3`\n",
    "\n",
    "Que remarquez-vous ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, all_w, all_b = train_linreg(n_epochs=7, batch_size=25, optim_params=dict(lr=5e-2))\n",
    "p1.training_evolution(all_w, all_b, loss_trace=loss_trace, data_trace=data_trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, all_w, all_b = train_linreg(n_epochs=7, batch_size=5, optim_params=dict(lr=1e-2))\n",
    "p1.training_evolution(all_w, all_b, loss_trace=loss_trace, data_trace=data_trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, all_w, all_b = train_linreg(n_epochs=15, batch_size=1, optim_params=dict(lr=1e-3))\n",
    "p1.training_evolution(all_w, all_b, loss_trace=loss_trace, data_trace=data_trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 2 - À chaque batch son minimum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Mettre en valeur quelques batches\n",
    "\n",
    "**TODO:**  \n",
    "Pour les 4 premiers lots avec une `batch_size` de 10, calculer le minimum associé avec `Polynomial.fit`, et les stocker. Utiliser ensuite la fonction d'affichage `training_evolution` pour afficher ces minima et les modèles associés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO générer le dataloader\n",
    "\n",
    "n_batches = 4  # le nombre de batches à afficher\n",
    "\n",
    "# grâce à ce zip, le `for` s'arrête après au plus `n_batches` itérations\n",
    "for _, (x_batch, y_batch) in zip(range(n_batches), dataloader):\n",
    "    pass  # TODO calculer le minimum du batch et le stocker dans des listes bien choisies\n",
    "\n",
    "# TODO gérer l'affichage avec `p1.training_evolution`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Afficher la *loss* de plusieurs lots\n",
    "\n",
    "**TODO:**  \n",
    "Compléter la fonction `plot_loss_batches` ci-dessous pour afficher le profil de *loss* pour les premiers *batches* sur une grille, en utilisant la fonction `p1.init_contour_trace`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.linspace(-10, 10, 25)\n",
    "b = np.linspace(-10, 10, 15)\n",
    "\n",
    "\n",
    "def plot_loss_batches(dataset, rows=3, cols=3, batch_size=10):\n",
    "    pass  # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:**  \n",
    "Faire ce type d'affichage avec différents `batch_size` pour comparer, notamment `batch_size=1` et `batch_size=5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_batches(dataset, batch_size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 3 - Descente avec inertie\n",
    "\n",
    "Pour s'extraires des minimas locaux, on peut ajouter un terme d'inertie à la descente de gradient. Cela permet d'explorer un peu plus l'espace des paramètres, en espérant trouver un minimum global, et de diminuer l'impact des différences entres les lots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. L'algorithme Adam\n",
    "\n",
    "**TODO:**  \n",
    "Reprendre l'algorithme de descente précédent, mais en utilisant l'optimiseur `torch.optim.Adam`, avec un *learning rate* de 1 et une *batch size* de 10. Comparer les résultats obtenus.\n",
    "\n",
    "*Remarque :* On peut aussi préciser un argument `momentum` pour l'inertie dans `torch.optim.SGD`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Sauvegarde du meilleur modèle\n",
    "\n",
    "Le risque de ces méthodes est de s'extraire du minimum global. Pour éviter cela, on peut garder un œil sur la loss moyenne pendant une époque, et sauvegarder le modèle qui a la meilleure performance.\n",
    "\n",
    "**TODO:**  \n",
    "Modifier les fonctions\n",
    "1. `train_epoch` pour calculer et renvoyer également la loss moyenne sur l'*epoch*.\n",
    "2. `train_linreg` pour sauvegarder le meilleur modèle au cours de l'entraînement.\n",
    "\n",
    "On pourrait utiliser `torch.save` pour sauvegarder le modèle dans un fichier, mais ici on se contentera de le stocker dans une variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuralnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
