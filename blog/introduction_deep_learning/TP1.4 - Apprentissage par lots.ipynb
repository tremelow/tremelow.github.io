{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet 1, TP4 - Apprentissage par lots\n",
    "\n",
    "On continue à suivre [le tutoriel PyTorch](https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html#full-implementation). On remarque notamment que la boucle d'apprentissage ressemble à la notre :\n",
    "\n",
    "```py\n",
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "```\n",
    "\n",
    "Cependant lorsqu'on regarde l'implémentation de `train_loop`, il contient une autre boucle,\n",
    "\n",
    "```py\n",
    "for batch, (X, y) in enumerate(dataloader):\n",
    "```\n",
    "\n",
    "C'est quoi un `dataloader`? Et pourquoi est-ce qu'il y a une boucle dans la boucle ?\n",
    "\n",
    "Précédemment, on a donné fait une descente de gradient en prenant toutes les données en entrée du réseau. \n",
    "En pratique, pour beaucoup d'applications (par exemple des images), cette quantité d'information est bien trop importante pour la mémoire de notre ordinateur, et il faut traiter les données **par lots** (par *batches*).\n",
    "Ainsi, dans le tutoriel, on trouve une ligne `train_dataloader = DataLoader(training_data, batch_size=64)`, qui initialise un itérable qui sert à obtenir un lot après l'autre, en l'occurrence avec des sous-échantillons de taille 64.\n",
    "Un passage complet dans cette boucle interne s'appelle une **époque** (ou *epoch*).\n",
    "\n",
    "Dans ce TP, on va étudier ce que ce comportement change \n",
    "\n",
    "*Remarque :* Le tutoriel contient aussi un `test_loop` et un `test_dataloader`. Ici, on n'a pas besoin de distinguer les jeux de données d'«entraînement» et de «validation», parce que notre **représentation** est très bien choisie. Il n'y a pas de risque d'*overfitting*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quelques imports utiles\n",
    "\n",
    "import numpy as np\n",
    "from numpy.polynomial.polynomial import Polynomial\n",
    "\n",
    "import plotly\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import projet1 as p1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data, y_data = p1.get_dataset()\n",
    "dataset = TensorDataset(x_data, y_data)\n",
    "\n",
    "slope_plot = np.linspace(-10, 10, 40)\n",
    "bias_plot = np.linspace(-10, 10, 30)\n",
    "loss_plot = p1.build_loss(slope_plot, bias_plot, x_data, y_data)\n",
    "\n",
    "data_trace = p1.data_scatter_trace(x_data, y_data)\n",
    "loss_trace = p1.init_contour_trace(slope_plot, bias_plot, loss_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 1 - Introduction aux batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Apprentissage sur une époque\n",
    "\n",
    "**TODO:**  \n",
    "Modifier cette fonction pour sauvegarder les valeurs des paramètres (pente et biais) du modèle après chaque pas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(dataloader, model, loss_fun, optimizer):\n",
    "    for x_batch, y_batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y_pred = model(x_batch)\n",
    "        batch_loss = loss_fun(y_pred, y_batch)\n",
    "\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #TODO sauvegarde des valeurs des paramètres\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. La boucle d'apprentissage\n",
    "\n",
    "**TODO:**  \n",
    "Construire un *dataloader* avec des lots de la même taille que le nombre de données et appliquer une boucle d'apprentissage sur 20 époques. Vérifier que les résultats correspondent à la boucle d'apprentissage sans *dataloader* du TP précédent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_linreg(\n",
    "    n_epochs=100, batch_size=0, optim_algo=torch.optim.SGD, model=None, **optim_params\n",
    "):\n",
    "    if model is None:\n",
    "        model = p1.LinearRegression()\n",
    "\n",
    "    dataset = TensorDataset(*p1.get_dataset())\n",
    "    if batch_size == 0:\n",
    "        batch_size = len(dataset)\n",
    "\n",
    "    #TODO définir le dataloader\n",
    "\n",
    "    loss_fun = nn.MSELoss()\n",
    "    optimizer = optim_algo(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    w_history, b_history = [], []\n",
    "    for _ in range(n_epochs):\n",
    "        w_epoch, b_epoch = train_epoch(dataloader, model, loss_fun, optimizer)\n",
    "        #TODO enregistrer l'historique des paramètres\n",
    "\n",
    "    return model, w_history, b_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Des lots de taille maximale\n",
    "\n",
    "**TODO:**  \n",
    "Vérifier qu'avec `batch_size=0`, on retrouve bien les résultats du TP précédent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, all_w, all_b = train_linreg(n_epochs=15)\n",
    "p1.training_evolution(all_w, all_b, loss_trace=loss_trace, data_trace=data_trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. L'apprentissage par lots\n",
    "\n",
    "Faire tourner des apprentissages avec les paramètres suivants :\n",
    "\n",
    "- `n_epochs=7, batch_size=25, learning_rate=5e-2`\n",
    "- `n_epochs=7, batch_size=5, learning_rate=1e-2`\n",
    "- `n_epochs=15, batch_size=1, learning_rate=1e-3`\n",
    "\n",
    "Que remarquez-vous ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 2 - À chaque batch son minimum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Mettre en valeur quelques batches\n",
    "\n",
    "**TODO:**  \n",
    "Pour les 4 premiers lots avec une `batch_size` de 10, calculer le minimum associé avec `Polynomial.fit`, et les stocker. Utiliser ensuite la fonction d'affichage `training_evolution` pour afficher ces minima et les modèles associés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Afficher la loss de plusieurs batches\n",
    "\n",
    "\n",
    "**TODO:**  \n",
    "Pour différents batches, afficher la loss dans l'espace des paramètres. Que remarquez-vous ?\n",
    "Faire évoluer le `batch_size`, notamment prendre `batch_size=1` et `batch_size=5` pour comparer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.linspace(-10, 10, 25)\n",
    "b = np.linspace(-10, 10, 15)\n",
    "\n",
    "\n",
    "def plot_first_batches(dataset, rows=3, cols=3, batch_size=10):\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, generator=p1.torch_rng())\n",
    "    assert len(dataloader) >= rows * cols\n",
    "\n",
    "    fig = make_subplots(rows=rows, cols=cols)\n",
    "    min_w, min_b, mean_loss = [], [], 0.0\n",
    "    for idx, (x_batch, y_batch) in enumerate(dataloader):\n",
    "        i, j = divmod(idx, cols)\n",
    "        #TODO calculer la loss et l'afficher\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_first_batches(dataset, batch_size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 3 - Descente avec inertie\n",
    "\n",
    "Pour s'extraires des minimas locaux, on peut ajouter un terme d'inertie à la descente de gradient. Cela permet d'explorer un peu plus l'espace des paramètres, en espérant trouver un minimum global, et de diminuer l'impact des différences entres les lots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. L'algorithme Adam\n",
    "\n",
    "**TODO:**  \n",
    "Reprendre l'algorithme de descente précédent, mais en utilisant l'optimiseur `torch.optim.Adam`, avec un *learning rate* de 1 et une *batch size* de 10. Comparer les résultats obtenus.\n",
    "\n",
    "*Remarque :* On peut aussi préciser un argument `momentum` pour l'inertie dans `torch.optim.SGD`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Sauvegarde du meilleur modèle\n",
    "\n",
    "Le risque de ces méthodes est de s'extraire du minimum global. Pour éviter cela, on peut garder un œil sur la loss moyenne pendant une époque, et sauvegarder le modèle qui a la meilleure performance.\n",
    "\n",
    "**TODO:**  \n",
    "Reprendre l'algorithme précédent en implémentant cette sauvegarde avec `torch.save`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuralnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
